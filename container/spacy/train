#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from keras.optimizers import Adam

from sklearn import tree

from numpy import mean

# newwww keras
from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name = 'training'
training_path = os.path.join(input_path, channel_name)

import spacy
import random

# Example of training data. For me, I am training a spaCy model to do NER task.
TRAIN_DATA = [
    ('Amazon co ca', {'entities': [(0, 6, 'BRD')]}),
    ('AMZNMKTPLACE AMAZON CO', {'entities': [(13, 19, 'BRD')]}),
    ('APPLE COM BILL', {'entities': [(0, 5, 'BRD')]}),
    ('BOOKING COM New York City', {'entities': [(0, 7, 'BRD')]}),
    ('STARBUCKS Vancouver', {'entities': [(0, 9, 'BRD')]}),
    ('Uber BV', {'entities': [(0, 4, 'BRD')]}),
    ('Hotel on Booking com Toronto', {'entities': [(9, 16, 'BRD')]}),
    ('UBER com', {'entities': [(0, 4, 'BRD')]}),
    ('Netflix com', {'entities': [(0, 7, 'BRD')]})
]


# Function code is cited from the article: https://manivannan-ai.medium.com/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6
def train_spacy(data, iterations):
    TRAIN_DATA = data
    nlp = spacy.blank('en')  # create blank Language class
    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(iterations):
            print("Statring iteration " + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.2,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)
    return nlp


# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        input_files = [os.path.join(training_path, file) for file in os.listdir(training_path)]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        # raw_data = [ pd.read_csv(file) for file in input_files ]
        raw_data = [pd.read_csv(file) for file in input_files]
        train_data = pd.concat(raw_data)

        # cat=['Credit_History','Education','Gender','Loan_Status']
        # train_data.columns=cat

        train_data['Title'] = train_data.Name.str.extract(' ([A-Za-z]+)\.', expand=False)
        train_data['Family_members'] = train_data['SibSp'] + train_data['Parch']
        train_data.drop(["Name", "SibSp", "Parch", "Cabin", "Ticket", "PassengerId"], axis=1, inplace=True)
        train_data["Title"] = train_data["Title"].replace(
            ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
        train_data["Title"] = train_data["Title"].replace(['Mlle', 'Ms'], 'Miss')
        train_data["Title"] = train_data["Title"].replace(['Mme'], 'Mrs')
        train_data['Age'] = train_data['Age'].fillna(train_data.groupby(["Title"])['Age'].transform('mean'))
        train_data['Fare'] = train_data['Fare'].fillna(train_data.groupby(["Pclass"])['Fare'].transform('mean'))
        train_data['Embarked'] = train_data['Embarked'].fillna("S")
        train_data.loc[(train_data['Pclass'] == 1), 'Pclass_Band'] = 3
        train_data.loc[(train_data['Pclass'] == 3), 'Pclass_Band'] = 1
        train_data.loc[(train_data['Pclass'] == 2), 'Pclass_Band'] = 2
        train_data = pd.get_dummies(train_data, columns=["Sex", "Embarked", "Title"])
        train_data.drop(["Sex_male", "Embarked_S"], axis=1, inplace=True)
        train_data["Title_Mr"] = 0
        train_data["Pclass_Band"] = train_data["Pclass_Band"].astype(int)
        train_data.drop(["Pclass"], axis=1, inplace=True)
        train_data["Survived"] = train_data["Survived"].astype(int)
        # log a metric on performance of the model     

        Y_train = train_data['Survived']
        X_train = train_data.drop('Survived', axis=1).values
        model = Sequential()

        model.add(Dense(16, input_dim=12, activation='relu'))
        model.add(Dense(8, input_dim=12, activation='relu'))
        model.add(Dense(4, activation='relu'))
        model.add(Dense(1, activation='sigmoid'))

        model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])
        clf = model.fit(X_train, Y_train, epochs=10, batch_size=8)

        _, accuracy = model.evaluate(X_train, Y_train)
        print('Training Accuracy: %.2f' % (accuracy * 100))
        # H5 model to delpoy model
        # save the model

        model.save(os.path.join(model_path, 'keras.h5'))
        # train_data.to_csv(os.path.join(model_path,"abc.csv"))
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)


if __name__ == '__main__':
    # train()

    # A zero exit code causes the job to be marked a Succeeded.
    # sys.exit(0)

    prdnlp = train_spacy(TRAIN_DATA, 20)

    # Save our trained Model
    modelfile = input("Enter your Model Name: ")
    prdnlp.to_disk(modelfile)

    # Test your text
    test_text = input("Enter your testing text: ")
    doc = prdnlp(test_text)
    for ent in doc.ents:
        print(ent.text, ent.start_char, ent.end_char, ent.label_)
